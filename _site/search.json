[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS626-GAA",
    "section": "",
    "text": "Welcome to my portfolio for Geospatial Analytics and Applications module in Singapore Management University!\nIn this website, you will find my hands-on exercise, in-class exercise as well as coursework for this course."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 1a",
    "section": "",
    "text": "Objective of this exercise:\n\nCreate thematic map, which involve the use of map symbols to visualize geographic features that are not naturally visible (e.g. population,temperature, crime rate, property prices, etc).\nCreate geovisualization, which uses visual representations and cartographic techniques to explore, analyze and communicate geospatial data"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1a",
    "section": "",
    "text": "Objective of this exercise:\n\nCreate thematic map, which involve the use of map symbols to visualize geographic features that are not naturally visible (e.g. population,temperature, crime rate, property prices, etc).\nCreate geovisualization, which uses visual representations and cartographic techniques to explore, analyze and communicate geospatial data"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#import-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#import-libraries",
    "title": "Hands-on Exercise 1a",
    "section": "Import Libraries",
    "text": "Import Libraries\n\npacman::p_load(sf, tmap, tidyverse, rvest)\n\nThe libraries used in this exercise would be:\n\ntmap: used to draw thematic map\nreadr: fast way to read rectangular data from delimited files (e.g. csv and tsv)\ntidyr: tool to create tidy data\ndplyr: grammar of data manipulation (to work with data frame like objects)\nsf: simple features in R to encode and analyze spatial vector data\nrvest: help scrape data from web page"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#the-data",
    "title": "Hands-on Exercise 1a",
    "section": "The Data",
    "text": "The Data\nIn this hands-on exercise, we’ll be using the Master PLan 2019 Master Boundary (No Sea) KML file (geospatial data), which consist of the geographical boundary of Singapore at planning subzone area in 2019.\nWe’ll also be using the Singapore Residents by Planning Area/Subzone, Age Group, Sex and Type of Dwelling, June 2024 in csv file (aspatial data). The PA and SZ column can be used to identify to georeference to the Master Plan 2019 Subzone Boundary data."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#import-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#import-data",
    "title": "Hands-on Exercise 1a",
    "section": "Import Data",
    "text": "Import Data\n\nmpsz &lt;- st_read(\"data/MasterPlan2019SubzoneBoundaryNoSeaKML.kml\")\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `C:\\stefanie-fel\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\MasterPlan2019SubzoneBoundaryNoSeaKML.kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html",
    "title": "Hands-on Exercise 1b",
    "section": "",
    "text": "Objective of this exercise is to import, wrangle, integrate and process geospatial data sets using:\n\nsf library to import geospatial data\nreadr library to import aspatial data\nUse Base R adnd sf libraries yto explore contents fof simple feature data frame\nUse sf library to transform or assign coordinate systems\nConvert aspatial data into sf data frame and perform geoprocessing tasks using sf library\nUse dpylr library to do data wrangling\nPerform EDA using ggplot2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-preparation",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#data-preparation",
    "title": "Hands-on Exercise 1a",
    "section": "Data Preparation",
    "text": "Data Preparation\nFirst we’ll create a function to extract values from HTML description\n\nextract_kml_field &lt;- function(html_text, field_name) {\n  if (is.na(html_text) || html_text == \"\") return(NA_character_)\n  \n  page &lt;- read_html(html_text)\n  rows &lt;- page %&gt;% html_elements(\"tr\")\n  \n  value &lt;- rows %&gt;%\n    keep(~ html_text2(html_element(.x, \"th\")) == field_name) %&gt;%\n    html_element(\"td\") %&gt;%\n    html_text2()\n  \n  if (length(value) == 0) NA_character_ else value\n}\n\n\n\n\n\n\n\nWhat does the code do\n\n\n\n\nThe function checks if html_text is an empty string and returns NA_character_\nIt parses the raw HTML string to HTML document and extract all table row element\nFor each row, it extracts the table row’s text and keeps only rows which table header matches field_name\nThen it returns the text from the table data cell\n\n\n\n\nmpsz &lt;- mpsz %&gt;%\n  mutate(\n    REGION_N = map_chr(Description, extract_kml_field, \"REGION_N\"),\n    PLN_AREA_N = map_chr(Description, extract_kml_field, \"PLN_AREA_N\"),\n    SUBZONE_N = map_chr(Description, extract_kml_field, \"SUBZONE_N\"),\n    SUBZONE_C = map_chr(Description, extract_kml_field, \"SUBZONE_C\")\n  ) %&gt;%\n  select(-Name, -Description) %&gt;%\n  relocate(geometry, .after = last_col())\n\n\n\n\n\n\n\nWhat does the code do\n\n\n\n\nExtracts fields from description column and use extract_kml_fieldto extract data from REGION_N, PLN_AREA_N, SUBZONE_N and SUBZONE_C\nMoves the geometry column to the very end using relocate\n\n\n\n\nmpsz\n\nSimple feature collection with 332 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\nFirst 10 features:\n         REGION_N    PLN_AREA_N           SUBZONE_N SUBZONE_C\n1  CENTRAL REGION   BUKIT MERAH          DEPOT ROAD    BMSZ12\n2  CENTRAL REGION   BUKIT MERAH         BUKIT MERAH    BMSZ02\n3  CENTRAL REGION        OUTRAM           CHINATOWN    OTSZ03\n4  CENTRAL REGION DOWNTOWN CORE             PHILLIP    DTSZ04\n5  CENTRAL REGION DOWNTOWN CORE       RAFFLES PLACE    DTSZ05\n6  CENTRAL REGION        OUTRAM        CHINA SQUARE    OTSZ04\n7  CENTRAL REGION   BUKIT MERAH         TIONG BAHRU    BMSZ10\n8  CENTRAL REGION DOWNTOWN CORE    BAYFRONT SUBZONE    DTSZ12\n9  CENTRAL REGION   BUKIT MERAH TIONG BAHRU STATION    BMSZ04\n10 CENTRAL REGION DOWNTOWN CORE       CLIFFORD PIER    DTSZ06\n                         geometry\n1  MULTIPOLYGON (((103.8145 1....\n2  MULTIPOLYGON (((103.8221 1....\n3  MULTIPOLYGON (((103.8438 1....\n4  MULTIPOLYGON (((103.8496 1....\n5  MULTIPOLYGON (((103.8525 1....\n6  MULTIPOLYGON (((103.8486 1....\n7  MULTIPOLYGON (((103.8311 1....\n8  MULTIPOLYGON (((103.8589 1....\n9  MULTIPOLYGON (((103.8283 1....\n10 MULTIPOLYGON (((103.8552 1...."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#import-and-prepare-data-for-the-aspatial-dataset",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#import-and-prepare-data-for-the-aspatial-dataset",
    "title": "Hands-on Exercise 1a",
    "section": "Import and Prepare Data for the Aspatial Dataset",
    "text": "Import and Prepare Data for the Aspatial Dataset\n\npopdata &lt;- read_csv(\"data/respopagesextod2024.csv\")\n\nWe want to prepare a data table with these variables:\n\nYOUNG: Age 0 to 4 - Age 20 to 24\nECONOMY ACTIVE: Age 25 to 29 - Age 60 - 64\nAGED: Age 65 and above\nTOTAL: sum the frequency of Age\nDEPENDENCY: the ratio between young +aged /economy active group\n\nFirst, we’ll use pivot_wide to transform long data format to wide format, whereas mutate(), filter(), group_by() and select() will be used to manipulate and filter data.\n\npopdata2024 &lt;- popdata %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nBefore performing the geospatial join, we’ll convert values of PA and SZ columns to uppercase, because it’s the column values are a mix of uppercase and lowercase. There will be a mismatch if the format isn’t changed.\n\npopdata2024 &lt;- popdata2024 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = list(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\nWe’ll use left_join to join both data and attribute table using SUBZONE_N and SZ as common identifier.\n\nmpsz_pop2024 &lt;- left_join(mpsz, popdata2024,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nNext, we’ll save the R object into file in binary format (RDS) using write_rds\n\nwrite_rds(mpsz_pop2024, \"data/mpszpop2024.rds\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#map-choropleth-with-tmap",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01.html#map-choropleth-with-tmap",
    "title": "Hands-on Exercise 1a",
    "section": "Map Choropleth With tmap",
    "text": "Map Choropleth With tmap\nChoropleth map is a type of a thematic map where the areas are shaded to indicate the value of a specific variable (e.g. demographics, economics, health, etc).\nThere are two ways we can create a choropleth map in R:\n\nWe can plot a simple choropleth using qtm()\nWe can also plot customizable thematic map using tmap\n\n\nMap Choropleth with qtm()\n\ntmap_mode(\"plot\")\nqtm(shp = mpsz_pop2024, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nWhat does this code do:\n\ntmap_mode(plot) is used to produce a static map, we can use tmap_mode(view) for producing interactive map\nfill argument is used to map attribute, in this case DEPENDENCY\n\n\n\nMap Thematic Map using tmap\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                n = 5,\n                values = \"brewer.blues\"),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_title(\"Distribution of Dependency Ratio by planning subzone\") +\n  tm_layout(frame = TRUE) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nWe can draw the choropleth step-by-step by drawing the base map first. We’ll use the tm_shape to define the input data and tm_polygon() to draw the planning subzone polygone. Note that apart from the ones we use above, there are also other functions i.e. tm_symbols, tm_lines, tm_raster() and tm_text().\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons()\n\n\n\n\n\n\n\n\nNext, we’ll add our target variable in the fill argument in the tm_polygon.\n\ntm_shape(mpsz_pop2024)+\n  tm_polygons(fill = \"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThe tm_polygon() is actually a wrapper of tm_fill() and tm_border(). tm_fill() shades the polygon using the color scheme and tm_borders() add border to the polygon, or else it will look like this.\n\ntm_shape(mpsz_pop2024)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\n\n\nThe tm_borders() has a few argument we can use:\n\nfill_alpha : define level of transparency (0 to 1, where 0 is transparent and 1 is not transparent)\ncol : border color\nlwd : border line width (default: 1)\nlty : border line type (default: solid)\n\nIn this example, we’ll use the col, lwd and lty argument.\n\ntm_shape(mpsz_pop2024)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(col = \"grey60\",\n             lwd = 0.1,\n             lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\nData Classification of tmap\nChoropleth maps usually use some method of data classification to take number of observations and bin them based on data ranges or classes. Depending on what kind of data classification used, the choropleth map can look different. tmap provides 10 classification methods, such as fixed, sd, equal, pretty (default, quantile, kmeans, hclust, bclust, fisher and jenks.\nIn this example, we’ll use quantile data classification that use 5 classes\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nIn this example, we’ll use equal data classification\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nIn this example, we’ll use quantile data classification that use 2 classes.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 2)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nNext, we’ll try the quantile data classification with 6 classes.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 6)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nIn this example, we’ll try quantile data classification using 10 classes\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 10)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nLastly, we’ll compare using 20 classes.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 20)) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nCreate Custom Breakpoints in Chroropleth\nWe can override the default values by using the breaks argument in the tm_scale_intervals() . Note that the breaks include minimum and maximum, so if we have n categories, we must specify n+1 elements in the breaks option (from smallest value to highest value). we can first check the descriptive statistics of the DEPENDENCY field to apply it to our plot.\n\nsummary(mpsz_pop2024$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1905  0.7450  0.8377  0.8738  0.9366 12.7500      94 \n\n\nAs can be seen, we can set break point at 0.60, 0.70, 0.80 and 0.90 and include min of 0 and max of 100.\n\ntm_shape(mpsz_pop2024)+\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00))) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nColor Scheme in tmap\nColor schemes in tmap are either user defined or using RColorBrewer package.\n\nUsing ColorBrewer Palette\nTo use the ColorBrewer palette, we can specify in the values argument.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"brewer.greens\")) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nWe can see that it’s shaded in green. But we can also reverse this by adding a “-” prefix in front of the palette.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"-brewer.greens\")) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nCartographic Feature in tmap\nWe can also draw other map furniture (e.g. compass, scale bar and grid lines). In this example, we’ll be using tm_compass(), tm_scale_bar(), tm_grod() and tm_credit() to draw a compass, scale bar, grid lines and data sources to the map.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5)) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: data.gov.sg & singstat\",\n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\n\n\nMap Layout\nMap layout refers to combination of all aethetic map elements to a cohesive map (e.g. background, frame, typography, scale, aspect ratio, etc) using tm_layout(). In this example, we can change teh placement, format and look of the legend.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5),\n              fill.legend = tm_legend(\n                title = \"Dependency ratio\")) +\n  tm_pos_auto_in() +\n  tm_borders(fill_alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scalebar() +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: data.gov.sg & singstat\",\n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nWhile in example, we can change layout settings to be changed using tmap_style() like below.\n\ntm_shape(mpsz_pop2024) +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\",\n                n = 5,\n                values = \"-brewer.greens\")) + \n  tm_borders(fill_alpha = 0.5) + \n  tmap_style(\"natural\")\n\n\n\n\n\n\n\n\n\n\n\nDrawing Small Choropleth Maps\nWe can create multiple small maps or called facet maps to visualize the spatial relationship change with other variables (e.g. time). We can plot them in several ways:\n\nAssigning multiple values to one of the aesthetic arguments\nCreating multiple stand-alone maps with tmap_arrange()\nDefining a group-by variable in tm_facets()\n\nIn this example, we’ll map multiple choropleth map by assigning two variables to the fill variable.\n\ntm_shape(mpsz_pop2024) + \n  tm_polygons(\n    fill = c(\"YOUNG\", \"AGED\"),\n    fill.legend = \n      tm_legend(position = tm_pos_in(\n        \"right\", \"bottom\")),\n    fill.scale = tm_scale_intervals(\n      style = \"equal\", \n      n = 5,\n      values = \"brewer.blues\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tmap_style(\"natural\")\n\n\n\n\n\n\n\n\nNext, we’ll creating multiple choropleth maps using grid layout (tmap_arrange())\n\nyoungmap &lt;- tm_shape(mpsz_pop2024)+ \n  tm_polygons(fill = \"YOUNG\",\n              fill.legend = tm_legend(\n                position = tm_pos_in(\n                  \"right\", \"bottom\"),\n                  item.height = 0.8),\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                values = \"brewer.blues\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Distribution of young population\")\n                \nagedmap &lt;- tm_shape(mpsz_pop2024)+ \n  tm_polygons(fill = \"AGED\",\n              fill.legend = tm_legend(\n                position = tm_pos_in(\n                  \"right\", \"bottom\"),\n                item.height = 0.8),\n              fill.scale = tm_scale_intervals(\n              style = \"quantile\", \n              values = \"brewer.blues\")) +\n  tm_borders(fill_alpha = 0.5) +\n  tm_title(\"Distribution of aged population\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nLastly, we’ll use tm_facets to define group-by variable.\n\ntm_shape(mpsz_pop2024) +\n  tm_fill(fill = \"DEPENDENCY\",\n          fill.scale = tm_scale_intervals(\n            style = \"quantile\",\n            values = \"brewer.blues\")) + \n  tm_facets(by = \"REGION_N\",\n            nrow = 2, \n            ncols = 3,\n            free.coords=TRUE, \n            drop.units=TRUE) +\n  tm_layout(legend.show = TRUE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\nWe can also select geographical area of inetrest and plot teh choropleth only on selected region using filter() function\n\nmpsz_pop2024 %&gt;%\n  filter(REGION_N == \"CENTRAL REGION\") %&gt;%\n  tm_shape() +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                values = \"brewer.greens\"),\n              fill.legend = tm_legend()) +\n  tm_borders(fill_alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\nComplementing Thematic Map with Statistical Chart\nMaps and statistical chart complement each other by visually representing different aspects of the same data to provide comprehensive understanding on a certain subject and provide data narrative. With tmap, statistical chart can be added into the map visualization using fill.chat argument of map layer and lend chart.\n\nmpsz_pop2024 %&gt;%\n  filter(REGION_N == \"CENTRAL REGION\") %&gt;%\n  tm_shape() +\n  tm_polygons(fill = \"DEPENDENCY\",\n              fill.scale = tm_scale_intervals(\n                style = \"quantile\", \n                values = \"brewer.greens\"),\n              fill.legend = tm_legend(),\n              fill.chart = tm_chart_box()) +\n  tm_borders() +\n  tm_layout(asp = 0.8)\n\n\n\n\n\n\n\n\nWe can also improve the visual representation by highlighting and labeling the outliers.\n\nmpsz_selected &lt;- mpsz_pop2024 %&gt;%\n  filter(REGION_N == \"CENTRAL REGION\")\n\nstats &lt;- boxplot.stats(mpsz_selected$DEPENDENCY)\n\noutlier_vals &lt;- stats$out\n\noutlier_sf &lt;- mpsz_selected[mpsz_selected$DEPENDENCY %in% outlier_vals, ]\n\ntm_shape(mpsz_selected) +\n  tm_polygons(fill = \"DEPENDENCY\",\n          fill.scale = tm_scale_intervals(\n            style = \"quantile\", \n            values = \"brewer.blues\"),\n          fill.legend = tm_legend(),\n          fill.chart = tm_chart_box()) +\n  tm_borders(fill_alpha = 0.5) +\ntm_shape(outlier_sf) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_text(\"SUBZONE_N\", col = \"red\", size = 0.7) +\n  tm_layout(asp = 0.8)\n\n\n\n\n\n\n\n\n\n\nCreate Interactive Chropleth Map\nWe can also create interactive maps to allow user to interact and engage with the data instead of just looking at the map statically. This can be done in tmap using tmap_mode() like below.\n\nregion_selected &lt;- mpsz_pop2024 %&gt;%\n  filter(REGION_N == \"CENTRAL REGION\")\nregion_bbox &lt;- st_bbox(region_selected)\n\nstats &lt;- boxplot.stats(region_selected$DEPENDENCY)\noutlier_vals &lt;- stats$out\noutlier_sf &lt;- region_selected[region_selected$DEPENDENCY %in% outlier_vals, ]\n\ntmap_mode(\"view\")\ntm_shape(region_selected, \n         bbox = region_bbox) +\n  tm_fill(\"DEPENDENCY\",\n          id = \"SUBZONE_N\",\n          popup.vars = c(\n            \"Name\" = \"SUBZONE_N\", \n            \"Dependency\" = \"DEPENDENCY\")) +\n  tm_borders() +\n  tm_shape(outlier_sf) +\n  tm_borders(col = \"red\", lwd = 2)\n\n\n\n\n\n\nCurrently the display of the map could distract users from interacting with the data. So let’s use set_zoom_limits argument to limit the map where users can zoom in or out.\n\nregion_selected &lt;- mpsz_pop2024 %&gt;%\n  filter(REGION_N == \"CENTRAL REGION\")\nregion_bbox &lt;- st_bbox(region_selected)\n\nstats &lt;- boxplot.stats(region_selected$DEPENDENCY)\noutlier_vals &lt;- stats$out\noutlier_sf &lt;- region_selected[region_selected$DEPENDENCY %in% outlier_vals, ]\n\ntmap_mode(\"view\")\n\ntm_shape(region_selected, \n         bbox = region_bbox) +\n  tm_fill(\"DEPENDENCY\",\n          id = \"SUBZONE_N\",\n          popup.vars = c(\n            \"Name\" = \"SUBZONE_N\", \n            \"Dependency\" = \"DEPENDENCY\")) +\n  tm_borders() +\n  tm_shape(outlier_sf) +\n  tm_borders(col = \"red\", lwd = 2) +\n  tm_view(set_zoom_limits = c(12,14))"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#getting-started",
    "title": "Hands-on Exercise 1b",
    "section": "",
    "text": "Objective of this exercise is to import, wrangle, integrate and process geospatial data sets using:\n\nsf library to import geospatial data\nreadr library to import aspatial data\nUse Base R adnd sf libraries yto explore contents fof simple feature data frame\nUse sf library to transform or assign coordinate systems\nConvert aspatial data into sf data frame and perform geoprocessing tasks using sf library\nUse dpylr library to do data wrangling\nPerform EDA using ggplot2"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-libraries",
    "title": "Hands-on Exercise 1b",
    "section": "Import Libraries",
    "text": "Import Libraries\n\npacman::p_load(sf, readr, dplyr, tidyr, tidyverse, ggplot2)\n\nThe libraries used in this exercise would be:\n\nsf: simple features in R to encode and analyze spatial vector data\nreadr: fast way to read rectangular data from delimited files (e.g. csv and tsv)\ndplyr: grammar of data manipulation (to work with data frame like objects)\ntidyverse: collection of R packages that for data manipulation and visualization\ntidyr: tool to create tidy data\nggplot2: create graphics based on “the Grammar of Objects”"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#the-data",
    "title": "Hands-on Exercise 1b",
    "section": "The Data",
    "text": "The Data\nIn this hands-on exercise, we’ll be using the following datasets:\n\nMaster Plan 2014 Subzone Boundary (Web)\nPre-schools Location\nCycling Path\nListing data of Airbnb"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-data",
    "title": "Hands-on Exercise 1b",
    "section": "Import Data",
    "text": "Import Data\nData Prep"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#import-geospatial-data",
    "title": "Hands-on Exercise 1b",
    "section": "Import Geospatial Data",
    "text": "Import Geospatial Data\nIn this section, we’ll learn to import geosptial data using st_read().\n\nImport Polygon Data in Shapefile Format\nst_read of sf library will be used to import the Master Plan 2014 Subzone Boundary shapefile into R as polygon feature data frame. When importing the data dsn argument i sused to define data path and layer is used to provide shapefile name without extension.\n\nmpsz = st_read(dsn = \"data/MasterPlan2014SubzoneBoundaryWebSHP\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\stefanie-fel\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\MasterPlan2014SubzoneBoundaryWebSHP' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nFrom the message above, it reveals that the geospatial objects are multipolygon features and there are 323 multipolygon features and 15 fieds in the data frame. The data frame is in svy21 projected coordinated systems.\n\n\nImport Polyline Data in Shapefile Form\nSimilar to importing polygon feature data, we’ll be using st_read from sf library to import the dataset to line feature data frame\n\ncyclingpath = st_read(dsn = \"data/CyclingPath_Apr2025\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\stefanie-fel\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\CyclingPath_Apr2025' \n  using driver `ESRI Shapefile'\nSimple feature collection with 4651 features and 19 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11721.1 ymin: 27550.13 xmax: 42809.37 ymax: 49702.59\nProjected CRS: SVY21\n\n\nThe dataset reveals that there’s 4651 features and 19 fields and they are captured in multilinestring object.\n\n\nImport GIS Data in KML Format\n\npreschool = st_read(\"data/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\stefanie-fel\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex01\\data\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nFrom the import message above, we can see that the dataset is a point feature data frame and there’s 2290 features and 2 fields. The dataset is also set in wgs84 coordinate system."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#check-the-content-of-simple-feature-data-frame",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#check-the-content-of-simple-feature-data-frame",
    "title": "Hands-on Exercise 1b",
    "section": "Check the Content of Simple Feature Data Frame",
    "text": "Check the Content of Simple Feature Data Frame\nThere are a few approaches to check the content fo a dataframe, such as:\n\nUsing st_geomtery\nThrough using st_geometry, we can see the basic information of the feature class (e.g. type of geometry, geographic extent of the features and the coordinate system of the data\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nUse glimpse()\nglimpse() provide basic geospatial feature information as well as associated attribute information in the dataframe like the data type of each fields.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nUse head()\nHead() is used to reveal complete information of the feature object and it gives user flexibility whether to show the whole record of the dataframe, or only a few select (through the argument n).\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#plotting-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex01/Hands-on_Ex01b.html#plotting-geospatial-data",
    "title": "Hands-on Exercise 1b",
    "section": "Plotting Geospatial Data",
    "text": "Plotting Geospatial Data\nIn geospatial data science, it’s important to be able to visualize the geospatial feature as we’re working with maps and location which is difficult to understand without looking at it visually. We can plot in R like below.\n\nplot(mpsz)\n\n\n\n\n\n\n\n\nBut if we just want to show the map as it is, we can use st_geometry().\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\n\n\nWe can also choose the plot we want to display by specifying the attribute like below.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n\n\n\nWorking with Projection\nTo perform geoprocessing using geospatial datasets, we need to ensure that geospatial datasets are projected using similar coordinate system or known as projection transformation.\n\nAssign ESPG code to simple feature data frame\nWhen importing geospatial data is that coordinate system of source data was missing or wrongly assigned during importing process. We can check coordinate system of a dataframe using st_crs()\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAs can be seen at the end of the message, the EPSG code is 9001, which is not the EPSG code for svy21. So we can reset by using the st_set_crs()\n\nmpsz &lt;- st_set_crs(mpsz,3414)\n\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html",
    "title": "Hands-on Exercise 2a",
    "section": "",
    "text": "Objective of this exercise is to evaluate pattern or distribution of a set of points on a surface (e.g. events or business services or facilities). In this hands-on exercise, we’ll specifically explore first-order spatial point pattern analysis (1st SPPA).\nFirst-order Spatial Point Pattern Analysis focus on understanding intensity of points across a study area since they vary across different study area. But the analysis don’t consider the interactions between individual location of points and distributions. The analysis also answers questions like where are points are densely located, how spread out the point patterns are and if the density uniform or vary across space.\nIn this exercise, we’ll be using spatstat specifically to answer these questions:\n\nAre childcare centres in Singapore randomly distributed throughout the country ?\nIf not, which location have higher childcare centres ?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#getting-started",
    "title": "Hands-on Exercise 2a",
    "section": "",
    "text": "Objective of this exercise is to evaluate pattern or distribution of a set of points on a surface (e.g. events or business services or facilities). In this hands-on exercise, we’ll specifically explore first-order spatial point pattern analysis (1st SPPA).\nFirst-order Spatial Point Pattern Analysis focus on understanding intensity of points across a study area since they vary across different study area. But the analysis don’t consider the interactions between individual location of points and distributions. The analysis also answers questions like where are points are densely located, how spread out the point patterns are and if the density uniform or vary across space.\nIn this exercise, we’ll be using spatstat specifically to answer these questions:\n\nAre childcare centres in Singapore randomly distributed throughout the country ?\nIf not, which location have higher childcare centres ?"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#import-libraries",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#import-libraries",
    "title": "Hands-on Exercise 2a",
    "section": "Import Libraries",
    "text": "Import Libraries\n\npacman::p_load(sf, terra, spatstat, \n               tmap, rvest, tidyverse)\n\nThe libraries used in this exercise would be:\n\nsf: simple features in R to encode and analyze spatial vector data\ntmap: used to draw thematic map\nrvest: help scrape data from web page\nspatstat: spatial statistics with focus on analyzing spatial point patterns in SD (with some 3D support)\nterra: methods for spatial data analysis with vector (points. lines, polygons) and raster (grid) data\ntidyverse: collection of R packages that for data manipulation and visualization"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#the-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#the-data",
    "title": "Hands-on Exercise 2a",
    "section": "The Data",
    "text": "The Data\nIn this hands-on exercise, we’ll be using the following datasets:\n\nMaster Plan 2019 Subzone Boundary (No Sea)\nChild Care Sevices"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#import-and-wrangle-geospatial-data",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02a.html#import-and-wrangle-geospatial-data",
    "title": "Hands-on Exercise 2a",
    "section": "Import and Wrangle Geospatial Data",
    "text": "Import and Wrangle Geospatial Data\nFirst, we’ll import the Master Plan 2019 Subzone Boundary (No Sea) in kml file.\n\nmpsz_sf &lt;- st_read(\"data/Master Plan 2019 Subzone Boundary (No Sea) (KML).kml\") %&gt;% \n  st_zm(drop = TRUE, what = \"ZM\") %&gt;% st_transform(crs = 3414)\n\nReading layer `URA_MP19_SUBZONE_NO_SEA_PL' from data source \n  `C:\\stefanie-fel\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\Master Plan 2019 Subzone Boundary (No Sea) (KML).kml' \n  using driver `KML'\nSimple feature collection with 332 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nThe st_zm() is used to remove Z (elevation) and M (measure) dimensions from geospatial geometries.\nNext, we’ll build a function to extract attributes like REGION_N, PLN_AREA_N, SUBZONE_N and SUBZONE_C from HTML Description column in the KML file and filter out unwanted SUBZONE_N and PLN_AREA_N.\n\nextract_kml_field &lt;- function(html_text, field_name) {\n  if (is.na(html_text) || html_text == \"\") return(NA_character_)\n  \n  page &lt;- read_html(html_text)\n  rows &lt;- page %&gt;% html_elements(\"tr\")\n  \n  value &lt;- rows %&gt;%\n    keep(~ html_text2(html_element(.x, \"th\")) == field_name) %&gt;%\n    html_element(\"td\") %&gt;%\n    html_text2()\n  \n  if (length(value) == 0) NA_character_ else value\n}\n\n\nmpsz_sf &lt;- mpsz_sf %&gt;%\n  mutate(\n    REGION_N = map_chr(Description, extract_kml_field, \"REGION_N\"),\n    PLN_AREA_N = map_chr(Description, extract_kml_field, \"PLN_AREA_N\"),\n    SUBZONE_N = map_chr(Description, extract_kml_field, \"SUBZONE_N\"),\n    SUBZONE_C = map_chr(Description, extract_kml_field, \"SUBZONE_C\")\n  ) %&gt;%\n  select(-Name, -Description) %&gt;%\n  relocate(geometry, .after = last_col())\n\n\nmpsz_cl &lt;- mpsz_sf %&gt;%\n  filter(SUBZONE_N != \"SOUTHERN GROUP\",\n         PLN_AREA_N != \"WESTERN ISLANDS\",\n         PLN_AREA_N != \"NORTH-EASTERN ISLANDS\")\n\n\nwrite_rds(mpsz_cl, \n          \"data/mpsz_cl.rds\")\n\nNext, we’ll import the Childcare Service data.\n\nchildcare_sf &lt;- st_read(\"data/ChildCareServices.kml\") %&gt;% \n  st_zm(drop = TRUE, what = \"ZM\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `CHILDCARE' from data source \n  `C:\\stefanie-fel\\ISSS626-GAA\\Hands-on_Ex\\Hands-on_Ex02\\data\\ChildCareServices.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nMapping Geospatial Datasets\n\ntm_shape(mpsz_cl) +\n  tm_polygons(col = \"lightgrey\", border.col = \"white\") +\ntm_shape(childcare_sf) +\n  tm_dots(col = \"red\", size = 0.1)\n\n\n\n\n\n\n\n\nWe can also make it into interactive plot by using tmap_mode()\n\ntmap_mode('view')\ntm_shape(childcare_sf)+\n  tm_dots()\n\n\n\n\n\n\ntmap_mode('plot')\n\n\n\nGeospatial Data Wrangling\nspatstat relies on its own specific data structures like ppp (planar point pattern) for point data and owin to observe data on specific region (or called windows). In this section, we’ll convert sf objects into spatstat ppp and owin object.\n\nConvert Data Frames to ppp Class\nspatstat requires the point event data in ppp object form. We’ll use the as.ppp() to convert the dtaset to ppp format\n\nchildcare_ppp &lt;- as.ppp(childcare_sf)\n\nTo verify the object class, we’ll use the class() function.\n\nclass(childcare_ppp)\n\n[1] \"ppp\"\n\n\nWe’ll also take a look at the summary statistics\n\nsummary(childcare_ppp)\n\nMarked planar point pattern:  1925 points\nAverage intensity 2.417323e-06 points per square unit\n\nCoordinates are given to 11 decimal places\n\nMark variables: Name, Description\nSummary:\n     Name           Description       \n Length:1925        Length:1925       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n\nWindow: rectangle = [11810.03, 45404.24] x [25596.33, 49300.88] units\n                    (33590 x 23700 units)\nWindow area = 796335000 square units\n\n\n\n\nCreate owin Object\nOwin object is designed to represent polygonal region. We can do that by using as.owin() function to convert sf object to owin.\n\nsg_owin &lt;- as.owin(mpsz_cl)\n\nWe use class() to check the object class\n\nclass(sg_owin)\n\n[1] \"owin\"\n\n\nWe can now display the owin object using plot()\n\nplot(sg_owin)\n\n\n\n\n\n\n\n\n\n\nCombine owin and ppp Objects\nWe can finally combine both objects by using the code below\n\nchildcareSG_ppp = childcare_ppp[sg_owin]\n\n\nchildcareSG_ppp\n\nMarked planar point pattern: 1925 points\nMark variables: Name, Description \nwindow: polygonal boundary\nenclosing rectangle: [2667.54, 55941.94] x [21448.47, 50256.33] units\n\n\n\n\n\nClark-Evan Test for Nearest Neighbor Analysis\nClark-evans test is a statistical method that utilizes the mean nearest neighbor distance (taking the mean of distances between each point to its nearest neighbor) to check if a point pattern is clustered, random or uniformly spaced. The test uses aggregation index (R) to describe the pattern:\n\nR &lt; 1 signifies a clustered point pattern\nR = 1 signify random point pattern\n\nIn this section, we’ll be using Clark-evans test using clarkevans.test(), in which the test hypotheses are:\n\nH0: The distribution are randomly distributed\nH1: The distribution are not randomly distributed\n\nand the 95% Confidence Interval will be used. For more information on the full arguments available, click here.\n\nPerform Clark-Evans test without CSR\nWe can perform Clark-Evans test without CSR as the clarkevans.test() supports testing without CRS\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"))\n\n\n    Clark-Evans test\n    No edge correction\n    Z-test\n\ndata:  childcareSG_ppp\nR = 0.53532, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\nPerform Clark-Evans test with CSR\nIn this section, we’ll be using the MonteCarlo method, in which the test is computed by comparing the observed value of R to results obtained from nsim (i.e. 39, 99, 999) simulated realizations of Complete Spatial Randomness conditional on observed number of points.\n\nclarkevans.test(childcareSG_ppp,\n                correction=\"none\",\n                clipregion=\"sg_owin\",\n                alternative=c(\"clustered\"),\n                method=\"MonteCarlo\",\n                nsim=99)\n\n\n    Clark-Evans test\n    No edge correction\n    Monte Carlo test based on 99 simulations of CSR with fixed n\n\ndata:  childcareSG_ppp\nR = 0.53532, p-value = 0.01\nalternative hypothesis: clustered (R &lt; 1)\n\n\n\n\n\nKernel Density Estimation Method\nKernel Density Estimation is valuable tool for visualizing and analyzing first-order spatial point patterns and is usually used for EDA because it can be used to visualize and understand spatial data patterns by transforming point faya into continuous density surfaces that reveal clusters and variations in event occurrences without making prior assumptions about data distributions.\n\nWorking with Automatic Bandwidth Selection Method\nTo use kernel density, we’ll be using the density() function.\n\nkde_SG_diggle &lt;- density(\n  childcareSG_ppp,\n  sigma=bw.diggle,\n  edge=TRUE,\n  kernel=\"gaussian\") \n\n\n\n\n\n\n\nWhat does the code do\n\n\n\n\nbw.diggle() automatic bandwidth selection method. Ther are other methods e.g. bw.CvL(), bw.scott() and bw.ppl()\nkernel argument is the smoothing kernel. It is used for …, the defualt value is “gaussian, but there are other smoothing methods available e.g. ”epancechnikov”, “quartic” and “disc”\nedge argument applies edge correction (meaning kernel that overlap the boundary are renormalized so they don’t lose mass). the default value for this argument is FALSE\nThe function outputs an im class, which represents a 2D pixel image. It’s a class used to store and manipulate raster data, where the spatial domain is divide to a grid of rectangular pixels\n\n\n\nThe plot() function can be used to display the kernel density derived.\n\nplot(kde_SG_diggle)\n\n\n\n\n\n\n\n\nWe can use summary() function to print the summary report\n\nsummary(kde_SG_diggle)\n\nreal-valued pixel image\n128 x 128 pixel array (ny, nx)\nenclosing rectangle: [2667.538, 55941.94] x [21448.47, 50256.33] units\ndimensions of each pixel: 416 x 225.0614 units\nImage is defined on a subset of the rectangular grid\nSubset area = 669941961.12249 square units\nSubset area fraction = 0.437\nPixel values (inside window):\n    range = [-6.584123e-21, 3.063698e-05]\n    integral = 1927.788\n    mean = 2.877545e-06\n\n\nBefore moving to the next step, it’s good to know that we can retrieve the bandwidth used to compute the kde layer\n\nbw &lt;- bw.diggle(childcareSG_ppp)\nbw\n\n   sigma \n295.9712 \n\n\n\n\nRescaling KDE Values\nAs we saw in the summary, the output range was from 0 to 0.0003727443 which is a very small number. This is because the default unit of measurement of svy21 is in meters. So, we can rescale.ppp() to convert the unit of measurement to km.\n\nchildcareSG_ppp_km &lt;- rescale.ppp(\n  childcareSG_ppp, 1000, \"km\")\n\nWe can re-apply the function\n\nkde_childcareSG_km &lt;- density(childcareSG_ppp_km,\n                              sigma=bw.diggle,\n                              edge=TRUE,\n                              kernel=\"gaussian\")\n\nAnd plot the kde object again\n\nplot(kde_childcareSG_km)\n\n\n\n\n\n\n\n\n\n\n\nWorking with Different Automatic Bandwidth Methods"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02b.html",
    "href": "Hands-on_Ex/Hands-on_Ex02/Hands-on_Exercise02b.html",
    "title": "Hands-on Exercise 2b",
    "section": "",
    "text": "Getting Started\nObjective of this exercise is to import, wrangle, integrate and process geospatial data sets using:\nsf library to import geospatial data\nreadr library to import aspatial data\nUse Base R adnd sf libraries yto explore contents fof simple feature data frame\nUse sf library to transform or assign coordinate systems\nConvert aspatial data into sf data frame and perform geoprocessing tasks using sf library\nUse dpylr library to do data wrangling\nPerform EDA using ggplot2\nImport Libraries\n{r} pacman::p_load(sf, readr, dplyr, tidyr, tidyverse, ggplot2)\nThe libraries used in this exercise would be:\nsf: simple features in R to encode and analyze spatial vector data\nreadr: fast way to read rectangular data from delimited files (e.g. csv and tsv)\ndplyr: grammar of data manipulation (to work with data frame like objects)\ntidyverse: collection of R packages that for data manipulation and visualization\ntidyr: tool to create tidy data\nggplot2: create graphics based on “the Grammar of Objects”\nThe Data\nIn this hands-on exercise, we’ll be using the following datasets:\nMaster Plan 2014 Subzone Boundary (Web)\nPre-schools Location\nCycling Path\nListing data of Airbnb\nImport Geospatial Data\nIn this section, we’ll learn to import geosptial data using st_read().\nImport Polygon Data in Shapefile Format\nst_read of sf library will be used to import the Master Plan 2014 Subzone Boundary shapefile into R as polygon feature data frame. When importing the data dsn argument i sused to define data path and layer is used to provide shapefile name without extension."
  }
]